{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树与随机森林"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分类的基本概念\n",
    "\n",
    "基于树（Tree based）的学习算法在数据科学中是相当常见的。这些算法给预测模型赋予了准确性、稳定性以及易解释性。和线性模型不同，它们对非线性关系也能进行很好的映射。常见的基于树的模型有：决策树（decision trees），随机森林（random forest）和提升树（boosted trees）。 \n",
    "\n",
    "下图的例子解释了分类的基本方法：\n",
    "\n",
    "![classification](../fig/decision-tree/classification-example.png)\n",
    "\n",
    "## 理解决策树\n",
    "\n",
    "决策树是机器学习最基本的模型，在不考虑其他复杂情况下，我们可以用一句话来描述决策树：如果得分大于等于60分，那么你及格了。\n",
    "\n",
    "这是一个最最简单的决策树的模型，我们把及格和没及格分别附上标签，及格（1），没及格（0），那么得到的决策树是这样的:\n",
    "\n",
    "![及格和不及格的决策树](..//fig//random-forest//score.png)\n",
    "\n",
    "决策树是一种监督学习算法。它适用于类别和连续输入（特征）和输出（预测）变量。基于树的方法把特征空间划分成一系列矩形，然后给每一个矩形安置一个简单的模型（像一个常数）。从概念上来讲，它们是简单且有效的。首先我们通过一个例子来理解决策树。然后用一种正规分析方法来分析创建决策树的过程。考虑一个简单的借贷公司顾客的数据集合。我们给定了所有客户的查询账户余额、信用记录、任职年限和先前贷款状况。相关任务是预测顾客的风险等级是否可信。该问题可以使用下列决策树来解决： \n",
    "\n",
    "![credit tree](../fig/decision-tree/credit-tree.png)\n",
    "\n",
    "决策树其实就是按节点分类数据集的一种方法。\n",
    "\n",
    "但现实中的决策往往比判断及格和不及格复杂得多，例如判断银行客户的信用质量。\n",
    "\n",
    "决策树的主要技术细节是如何构建有关数据的问题，决策树是通过形成能够最大限度减少基尼系数的问题而建立的。\n",
    "\n",
    "## 决策树的分类\n",
    "\n",
    "常见的决策树算法主要有以下几种：\n",
    "\n",
    "1. ID3：使用信息增益g(D,A)进行特征选择\n",
    "2. C4.5：信息增益率 =g(D,A)/H(A)\n",
    "3. CART：基尼系数\n",
    "\n",
    "信息增益：一个特征的信息增益(或信息增益率，或基尼系数)越大，表明特征对样本的熵的减少能力更强，这个特征使得数据由不确定性到确定性的能力越强。\n",
    "\n",
    "\n",
    "## 流程\n",
    "\n",
    "创建决策树进行分类的流程如下：\n",
    "1. 创建数据集\n",
    "2. 计算数据集的信息熵\n",
    "3. 遍历所有特征，选择信息熵最小的特征，即为最好的分类特征\n",
    "4. 根据上一步得到的分类特征分割数据集，并将该特征从列表中移除\n",
    "5. 执行递归函数，返回第三步，不断分割数据集，直到分类结束\n",
    "6. 使用决策树执行分类，返回分类结果\n",
    "\n",
    "下面是一个简单的例子：\n",
    "\n",
    "![decision-example](../fig/decision-tree/decision-tree-example.png)\n",
    "\n",
    "数据解读：\n",
    "\n",
    "在该数据集中包含五个海洋动物，有两个特征：（1）不浮出水面是否可以生存；（2）是否有脚蹼；这些动物被分成两类：鱼类和非鱼类。在我们构建决策树的过程中，对某个动物，只有两个特征都为“是”时，才将其判定为鱼类。\n",
    "\n",
    "在构建决策树时，我们需要解决的第一个问题是：当前数据集哪个特征在划分数据分类时起决定性作用，即我们要如何找出最优的分类特征。为了找到决定性的特征，划分出最好的结果，我们必须评估每个特征。完成数据划分后，原始数据集就被划分为几个数据子集，这些数据子集会分布在第一个决策点的所有分支上。如果某个分支下的数据属于同一类型，即数据已正确分类，无需进一步分割。如果数据子集内的数据不属于同个类型，则需要重复划分数据子集的过程。划分数据子集的算法和划分原始数据集的方法相同（因此可用递归函数继续划分子集），直到所有具有相同类型的数据都在一个数据子集内。\n",
    "\n",
    "构建决策树的伪代码函数createTree()如下所示：\n",
    "\n",
    "```\n",
    "检测数据集中的每个子集是否属于同一分类：\n",
    "    If so return 类标签\n",
    "    Else:\n",
    "        寻找划分数据集的最好特征\n",
    "        划分数据集\n",
    "        创建分支节点\n",
    "            For 每个划分的子集：\n",
    "                调用函数createTree()并增加返回结果到分支节点中\n",
    "        Return 分支节点\n",
    "```\n",
    "\n",
    "（1）划分数据集的大原则是：将无序的数据变得更加有序。这里引入信息熵的概念。如果待分类的事务可能划分在多个分类之中，则符号xi的信息定义为：\n",
    "\n",
    "$$ I(x_i) = -\\log_2 p(x_i) $$\n",
    "\n",
    "其中$p(x_i)$是选择该分类的概率。\n",
    "\n",
    "为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值，通过下面的公式得到：\n",
    "\n",
    "$$ H = -\\sum_{i=1}^{n}{p(x_i)\\cdot \\log_2 p(x_i)} $$\n",
    "\n",
    "直观的理解：如果x属于某个分类的值越大（即数据越有序），H的值越小；极端情况下，p(xi)=1时，H=0，此时分类最准确。所以我们要使H的值尽可能小。\n",
    "\n",
    "（2）计算完信息熵后，我们便可以得到数据集的无序程度。我们将对每个特征划分数据集的结果计算一次信息熵，然后判断哪个特征划分数据集是最好的划分方式（根据信息熵判断，信息熵越小，说明划分效果越好）。\n",
    "\n",
    "（3）选择最好的数据集划分方式\n",
    "\n",
    "（4）选择好最好的划分特征后，接下来，可以开始创建决策树了。其工作原理如下：得到原始数据集，然后基于最好的属性值划分数据集，由于特征值可能多于两个，因此可能存在大于两个分支的数据集划分。第一次划分后，数据将被向下传递到树分支的下一个节点，在这个节点上，我们可以再次划分数据。因此我们可以使用递归的原则处理数据集。递归结束的条件是：程序遍历完所有划分数据集的属性，或者每个分支下的所有实例都具有相同的分类。\n",
    "\n",
    "（5）依靠训练数据构造了决策树之后，我们可以将它用于实际数据的分类。在执行数据分类时，需要使用决策树以及用于构造树的标签向量。然后，程序比较测试数据与决策树上的数值，递归执行该过程直到进入叶子节点；最后将测试数据定义为叶子节点所属的类型。\n",
    "\n",
    "我们可以看到，只有测试数据的两个特征都为1时，才会输出‘yes’，判定为鱼类，结果符合我们的实际要求。\n",
    "\n",
    "现在我们已经创建了使用决策树的分类器，但是每次使用分类器时，必须重新构造决策树，而且构造决策树是很耗时的任务。因此，为了节省计算时间，最好能够在每次执行分类时调用已经构造好的决策树。这里我们使用Python的pickle模块序列化对象。序列化对象可以在磁盘上保存对象，并在需要的时候读取出来。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 例：美国的收入数据\n",
    "\n",
    "本例子参见：https://www.jiqizhixin.com/articles/2017-07-31-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in colab:\n",
    "# !pip install plotnine\n",
    "# !pip install  sklearn_pandas\n",
    "# !pip3 install scipy==1.2.0\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotnine import *\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "training_data = '../data/adult-training.csv'\n",
    "test_data = '../data/adult-test.csv'\n",
    "\n",
    "columns = ['Age','Workclass','fnlgwt','Education','EdNum','MaritalStatus','Occupation','Relationship','Race','Sex','CapitalGain','CapitalLoss','HoursPerWeek','Country','Income']\n",
    "\n",
    "df_train_set = pd.read_csv(training_data, names=columns)\n",
    "df_test_set = pd.read_csv(test_data, names=columns, skiprows=1)\n",
    "df_train_set.drop('fnlgwt', axis=1, inplace=True)\n",
    "df_test_set.drop('fnlgwt', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的代码中，我们首先需要导入所有需要的库和模块，然后再读取数据和结构到训练数据和验证数据中。我们同样去除 fnlgwt 列，因为该数据行对于模型的训练并不重要。\n",
    "\n",
    "输入以下语句可以看到训练数据的前五行： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如下所示，我们还需要做一些数据清洗。我们需要将所有列的的特殊字符移除，此外任何空格或者 \".\" 都需要移除。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the special character to \"Unknown\"\n",
    "for i in df_train_set.columns:\n",
    "    df_train_set[i].replace(' ?', 'Unknown', inplace=True)\n",
    "    df_test_set[i].replace(' ?', 'Unknown', inplace=True) \n",
    "for col in df_train_set.columns:\n",
    "  if df_train_set[col].dtype != 'int64':\n",
    "    df_train_set[col] = df_train_set[col].apply(lambda val: val.replace(\" \", \"\"))\n",
    "    df_train_set[col] = df_train_set[col].apply(lambda val: val.replace(\".\", \"\"))\n",
    "    df_test_set[col] = df_test_set[col].apply(lambda val: val.replace(\" \", \"\"))\n",
    "    df_test_set[col] = df_test_set[col].apply(lambda val: val.replace(\".\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如上图所示，有两行描述了个人的教育：Eduction 和 EdNum。我们假设这两个特征十分相关，因此我们可以移除 Education 列。Country 列对预测收入并不会起到什么作用，所以我们需要移除它。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_set.drop([\"Country\", \"Education\"], axis=1, inplace=True)\n",
    "df_test_set.drop([\"Country\", \"Education\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age 和 EdNum 列是数值型的，我们可以将连续数值型转化为更高效的方式，例如将年龄换为 10 年的整数倍，教育年限换为 5 年的整数倍，实现的代码如下： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = list(df_train_set.columns)\n",
    "colnames.remove('Age')\n",
    "colnames.remove('EdNum')\n",
    "colnames = ['AgeGroup', 'Education'] + colnames\n",
    "\n",
    "labels = [\"{0}-{1}\".format(i, i + 9) for i in range(0, 100, 10)]\n",
    "df_train_set['AgeGroup'] = pd.cut(df_train_set.Age, range(0, 101, 10), right=False, labels=labels)\n",
    "df_test_set['AgeGroup'] = pd.cut(df_test_set.Age, range(0, 101, 10), right=False, labels=labels)\n",
    "\n",
    "labels = [\"{0}-{1}\".format(i, i + 4) for i in range(0, 20, 5)]\n",
    "df_train_set['Education'] = pd.cut(df_train_set.EdNum, range(0, 21, 5), right=False, labels=labels)\n",
    "df_test_set['Education'] = pd.cut(df_test_set.EdNum, range(0, 21, 5), right=False, labels=labels)\n",
    "\n",
    "df_train_set = df_train_set[colnames]\n",
    "df_test_set = df_test_set[colnames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们已经清理了数据，下面语句可以展示我们数据的概况： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_set.Income.value_counts()\n",
    "<=50K    24720\n",
    ">50K      7841\n",
    "Name: Income, dtype: int64\n",
    "df_test_set.Income.value_counts()\n",
    "<=50K    12435\n",
    ">50K      3846\n",
    "Name: Income, dtype: int64"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "在训练集和测试集中，我们发现 <=50K 的类别要比>50K 的多 3 倍。从这里我们就可以看出来样本数据并不是均衡的数据，但是在这里为了简化问题，我们在这里将该数据集看作常规问题。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，让我们以图像的形式看一下训练数据中的不同特征的分布和相互依存（inter-dependence）关系。首先看一下关系（Relationships）和婚姻状况（MaritalStatus）特征是如何相互关联的。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ggplot(df_train_set, aes(x = \"Relationship\", fill = \"MaritalStatus\"))+ geom_bar(position=\"fill\")+ theme(axis_text_x = element_text(angle = 60, hjust = 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有很多关于性别对收入差距的影响的相关说法。我们可以分别看见男性和女性的教育程度和种族间的影响。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ggplot(df_train_set, aes(x = \"Education\", fill = \"Income\"))+ geom_bar(position=\"fill\")+ theme(axis_text_x = element_text(angle = -90, hjust = 1))+ facet_wrap('~Sex'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ggplot(df_train_set, aes(x = \"Race\", fill = \"Income\"))+ geom_bar(position=\"fill\")+ theme(axis_text_x = element_text(angle = -90, hjust = 1))+ facet_wrap('~Sex'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直到现在，我们仅关注了非数值特征（non-numeric）的相互关系。现在我们看一下资本收益（CapitalGain）和资本损失（CapitalLoss）对收入的影响。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ggplot(df_train_set, aes(x=\"Income\", y=\"CapitalGain\"))+ geom_jitter(position=position_jitter(0.1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ggplot(df_train_set, aes(x=\"Income\", y=\"CapitalLoss\"))+ geom_jitter(position=position_jitter(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 树分类器\n",
    "\n",
    "现在我们理解了我们数据中的一些关系，所以就可以使用 sklearn.tree.DecisionTreeClassifier 创建一个简单的树分类器模型。然而，为了使用这一模型，我们需要把所有我们的非数值数据转化成数值型数据。我们可以直接在 Pandas 数据框架中使用 sklearn.preprocessing.LabeEncoder 模块和 sklearn_pandas 模块就可以轻松地完成这一步骤。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = DataFrameMapper([('AgeGroup', LabelEncoder()),('Education', LabelEncoder()),('Workclass', LabelEncoder()),('MaritalStatus', LabelEncoder()),('Occupation', LabelEncoder()),('Relationship', LabelEncoder()),('Race', LabelEncoder()),('Sex', LabelEncoder()),('Income', LabelEncoder())], df_out=True, default=None)\n",
    "\n",
    "cols = list(df_train_set.columns)\n",
    "cols.remove(\"Income\")\n",
    "cols = cols[:-3] + [\"Income\"] + cols[-3:]\n",
    "\n",
    "df_train = mapper.fit_transform(df_train_set.copy())\n",
    "df_train.columns = cols\n",
    "\n",
    "df_test = mapper.transform(df_test_set.copy())\n",
    "df_test.columns = cols\n",
    "\n",
    "cols.remove(\"Income\")\n",
    "x_train, y_train = df_train[cols].values, df_train[\"Income\"].values\n",
    "x_test, y_test = df_test[cols].values, df_test[\"Income\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们用正确的形式对数据进行了训练和测试，已创建了我们的第一个模型！ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treeClassifier = DecisionTreeClassifier()\n",
    "treeClassifier.fit(x_train, y_train)\n",
    "treeClassifier.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最简单的且没有优化的概率分类器模型可以达到 83.5% 的精度。在分类问题中，混淆矩阵（confusion matrix）是衡量模型精度的好方法。使用下列代码我们可以绘制任意基于树的模型的混淆矩阵。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertoolsfrom sklearn.metrics import confusion_matrixdef plot_confusion_matrix(cm, classes, normalize=False):\"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    cmap = plt.cm.Blues\n",
    "    title = \"Confusion Matrix\"if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        cm = np.around(cm, decimals=3)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    thresh = cm.max() / 2.for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们可以看到第一个模型的混淆矩阵： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = treeClassifier.predict(x_test)\n",
    "cfm = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "plt.figure(figsize=(10,6))\n",
    "plot_confusion_matrix(cfm, classes=[\"<=50K\", \">50K\"], normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们发现多数类别（<=50K）的精度为 90.5%，少数类别（>50K）的精度只有 60.8%。\n",
    "\n",
    "让我们看一下调校此简单分类器的方法。我们能使用带有 5 折交叉验证的 GridSearchCV() 来调校树分类器的各种重要参数。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'max_features':(None, 9, 6),'max_depth':(None, 24, 16),'min_samples_split': (2, 4, 8),'min_samples_leaf': (16, 4, 12)}\n",
    "\n",
    "clf = GridSearchCV(treeClassifier, parameters, cv=5, n_jobs=4)\n",
    "clf.fit(x_train, y_train)\n",
    "clf.best_score_, clf.score(x_test, y_test), clf.best_params_\n",
    "(0.85934092933263717,\n",
    " 0.85897672133161351,\n",
    " {'max_depth': 16,\n",
    "  'max_features': 9,\n",
    "  'min_samples_leaf': 16,\n",
    "  'min_samples_split': 8})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过优化，我们发现精度上升到了 85.9%。在上方，我们也可以看见最优模型的参数。现在，让我们看一下 已优化模型的混淆矩阵（confusion matrix）： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(x_test)\n",
    "cfm = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "plt.figure(figsize=(10,6))\n",
    "plot_confusion_matrix(cfm, classes=[\"<=50K\", \">50K\"], normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过优化，我们发现在两种类别下，预测精度都有所提升。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的局限性\n",
    "\n",
    "决策树有很多优点，比如：\n",
    "- 易于理解、易于解释\n",
    "- 可视化\n",
    "- 无需大量数据准备。不过要注意，sklearn.tree 模块不支持缺失值。\n",
    "- 使用决策树（预测数据）的成本是训练决策时所用数据的对数量级。\n",
    "\n",
    "但这些模型往往不直接使用，决策树一些常见的缺陷是：\n",
    "\n",
    "- 构建的树过于复杂，无法很好地在数据上实现泛化。\n",
    "- 数据的微小变动可能导致生成的树完全不同，因此决策树不够稳定。\n",
    "- 决策树学习算法在实践中通常基于启发式算法，如贪婪算法，在每一个结点作出局部最优决策。此类算法无法确保返回全局最优决策树。\n",
    "- 如果某些类别占据主导地位，则决策树学习器构建的决策树会有偏差。因此推荐做法是在数据集与决策树拟合之前先使数据集保持均衡。\n",
    "- 某些类别的函数很难使用决策树模型来建模，如 XOR、奇偶校验函数（parity）和数据选择器函数（multiplexer）。\n",
    "\n",
    "大部分限制可以通过改善决策树轻易解决。在下面的内容中，我们将介绍相关的几个概念，重点介绍袋装和随机森林。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 剪枝\n",
    "\n",
    "由于决策树容易对数据产生过拟合，因此分支更少（即减少区域 R_1, … ,R_J）的小树虽然偏差略微高一点，但其产生的方差更低，可解释性更强。处理上述问题的一种方法是构建一棵树，每个分支超过某个（高）阈值造成叶结点误差率 Qm 下降，则结束构建。但是，由于分裂算法的贪婪本质，它其实很短视。决策树早期看似无用的一次分裂有可能会导致之后一次优秀的分裂，并使得 Qm 大幅下降。\n",
    "\n",
    "因此，更好的策略是构建一个非常大的树 T_0，然后再剪枝，得到一棵子树。剪枝可以使用多种策略。代价复杂度剪枝（Cost complexity pruning），又叫最弱连接剪枝（weakest link pruning），就是其中一种行之有效的策略。除了考虑每一个可能的子树之外，还需要考虑由非负调参（nonnegative tuning parameter）α 索引的树序列。每一个 α 值都对应一个尽可能小的子树 T⊂T_0。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 袋装（Bootstrap Aggregating——Bagging） \n",
    "\n",
    "在统计学中，Bootstrap 是依靠替换随机采样的任意试验或度量。我们从上文可以看见，决策树会受到高方差的困扰。这意味着如果我们把训练数据随机分成两部分，并且给二者都安置一个决策树，我们得到的结果可能就会相当不同。Bootstrap 聚集，或者叫做袋装，是减少统计学习方法的方差的通用过程。\n",
    "\n",
    "给定一组 n 个独立的样本观测值 Z_1，Z_2，...，Z_n，每一个值的方差均为 *σ^*2，样本观测值的均值方差为 *σ^*2/*n*。换句话说，对一组观测值取平均会减小方差。因此一种减小方差的自然方式，也就是增加统计学习方法预测精度的方式，就是从总体中取出很多训练集，使用每一个训练集创建一个分离的预测模型，并且对预测结果求取平均值。\n",
    "\n",
    "这里有一个问题，即我们不能获取多个训练数据集。相反，我们可以通过从（单一）训练数据集提取重复样本进行自助法（bootstrap）操作。在这种方法中，我们生成了 B 个不同的自助训练数据集。我们随后在第 b 个自助训练数据集得到了一个预测结果，从而获得一个聚集预测（aggregate prediction）。\n",
    "\n",
    "这就叫做袋装（bagging）。注意，聚集（aggregating）在回归和分类问题中可能有不同的均值。当平均预测值在回归问题中的效果很好时，我们将会需要使用多数票决（majority vote）：由于分类问题中的聚集机制，整体预测就是在 B 个预测值中最常出现的那个主要类别。 \n",
    "\n",
    "虽然袋装技术（Bagging）通过降低方差而提高了一般决策树的预测性能，但它还遇到了其他缺点：Bagging 要求我们在自助样本上生成整棵树，这就增加了 B 倍计算复杂度。此外，因为基于 Bagging 的树是相关联的，预测精度会根据 B 而饱和。\n",
    "\n",
    "随机森林通过随机扰动而令所有的树去相关，因此随机森林要比 Bagging 性能更好。随机森林不像 Bagging，在构建每一棵树时，每一个结点分割前都是采用随机样本预测器。因为在核心思想上，随机森林还是和 Bagging 树一样，因此其在方差上有所减少。此外，随机森林可以考虑使用大量预测器，不仅因为这种方法减少了偏差，同时局部特征预测器在树型结构中充当重要的决策。\n",
    "\n",
    "随机森林可以使用巨量的预测器，甚至预测器的数量比观察样本的数量还多。采用随机森林方法最显著的优势是它能获得更多的信息以减少拟合数值和估计分割的偏差。\n",
    "\n",
    "通常我们会有一些预测器能主导决策树的拟合过程，因为它们的平均性能始终要比其他一些竞争预测器更好。因此，其它许多对局部数据特征有用的预测器并不会选定作为分割变量。随着随机森林计算了足够多的决策树模型，每一个预测器都至少有几次机会能成为定义分割的预测器。大多数情况下，我们不仅仅只有主导预测器，特征预测器也有机会定义数据集的分割。\n",
    "\n",
    "## Out-of-Bag（OOB）误差 \n",
    "\n",
    "Bagging 方法最大的优势是我们可以不通过交叉验证而求得测试误差。回想一下，Bagging 方法的精髓是多棵树可以重复地拟合观察样本的自助子集。平均而言，每一个袋装树可以利用 2/3 的观察样本。而剩下的 1/3 观察样本就可以称为 out-of-bag (OOB) 观察样本，它们并不会拟合一一棵给定袋装树。我们可以使用每一棵树的 OOB 观察样本而计算第 i 个观察样本的预测值，这将会导致大约有 B/3 的预测值可以预测第 i 个观察样本。现在我们可以使用和 Bagging（平均回归和大多数投票分类）类似的聚集技术，我们能获得第 i 个观察样本的单一预测值。我们可以用这种方式获得 n 个观察样本的 OOB 预测，因此总体的 OOB MSE（回归问题）和分类误差率（分类问题）就能计算出来。OOB 误差结果是 Bagging 模型测试误差的有效估计，因为每一个样本的预测值都是仅仅使用不会进行拟合训练模型的样本。 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征重要性度量\n",
    "\n",
    "通过使用单一树，Bagging 通常会提升预测的精确度。但是，解释最终的模型可能很困难。当我们袋装大量的树时，就不再可能使用单一的树表征最终的统计学习流程，因此，Bagging 是以牺牲阐释性能力为代价来提升预测精确度的。有趣的是，一个人可使用 RSS（用于 bagging 回归树）或者基尼指数（用于 bagging 分类树）得到每一个预测器的整体总结。在 bagging 回归树的情况中，我们可以记录由于所有的 B 树上平均的给定预测分子分裂而造成的 RSS 减少的所有数量。一个大的值表示一个重要的预测器。相似地，在 bagging 分类树的情况下，我们可以添加由于所有的 B 树上平均的给定预测分子分裂而造成的基尼系数降低的所有数量。一旦训练完成，sklearn 模块的不同袋装树（bagged tree）学习方法可直接访问特征的重要性数据作为属性。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机森林有三个主要的超参数调整\n",
    "\n",
    "- 结点规模：随机森林不像决策树，每一棵树叶结点所包含的观察样本数量可能十分少。该超参数的目标是生成树的时候尽可能保持小偏差。\n",
    "- 树的数量：在实践中选择数百棵树一般是比较好的选择。\n",
    "- 预测器采样的数量：一般来说，如果我们一共有 D 个预测器，那么我们可以在回归任务中使用 D/3 个预测器数作为采样数，在分类任务中使用 D^(1/2) 个预测器作为抽样。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是一个随机生成的数据做随机森林模型的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "##创建100个类共10000个样本，每个样本10个特征\n",
    "X, y = make_blobs(n_samples=10000, n_features=10, centers=100,random_state=0)\n",
    "\n",
    "## 决策树\n",
    "clf1 = DecisionTreeClassifier(max_depth=None, min_samples_split=2,random_state=0)\n",
    "scores1 = cross_val_score(clf1, X, y)\n",
    "print(scores1.mean())\n",
    "\n",
    "## 随机森林\n",
    "clf2 = RandomForestClassifier(n_estimators=10, max_depth=None,min_samples_split=2, random_state=0)\n",
    "scores2 = cross_val_score(clf2, X, y)\n",
    "print(scores2.mean())\n",
    "\n",
    "## ExtraTree分类器集合\n",
    "clf3 = ExtraTreesClassifier(n_estimators=10, max_depth=None,min_samples_split=2, random_state=0)\n",
    "scores3 = cross_val_score(clf3, X, y)\n",
    "print(scores3.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回到刚才的收入数据，现在我们构建一个包含 500 棵树的简单随机森林分类器模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rclf = RandomForestClassifier(n_estimators=500)\n",
    "rclf.fit(x_train, y_train)\n",
    "rclf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即使没有任何优化，我们仍然发现模型性能可以和已优化决策树分类器相媲美，并且测试分达到了 85.1%。按照下面的混淆矩阵，我们发现简单的随机森林和经过优化的树型分类器表现差不多，其在主要类别（<=50K 收入）的预测精度达到了 92.1%，而在少数类别（>50K 收入）上达到了 62.6%。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rclf = RandomForestClassifier(n_estimators=500)\n",
    "rclf.fit(x_train, y_train)\n",
    "rclf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如前面所探讨的，随机森林模型还提供了特征重要性的度量方法。我们可以在下图中看到目前模型不同特征的重要性： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rclf.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "cols = [cols[x] for x in indices]\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), cols)\n",
    "plt.xlabel('Relative Importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们可以尝试优化我们的随机森林模型，如下我们可以使用带 5-折交叉验证的 GridSearchCV() 操作来优化随机森林： "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_estimators':(100, 500, 1000),'max_depth':(None, 24, 16),'min_samples_split': (2, 4, 8),'min_samples_leaf': (16, 4, 12)}\n",
    "\n",
    "clf = GridSearchCV(RandomForestClassifier(), parameters, cv=5, n_jobs=8)\n",
    "clf.fit(x_train, y_train)\n",
    "clf.best_score_, clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到现在的模型要显著地比前面的更好一些，并且预测率达到了 86.6%。按照下面的混淆矩阵，新模型在主要类别的预测精度上有显著的提升，并且在少数类别的预测上精度只稍微降低了一点。这是非平衡数据普遍存在的问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rclf2 = RandomForestClassifier(n_estimators=1000,max_depth=24,min_samples_leaf=4,min_samples_split=8)\n",
    "rclf2.fit(x_train, y_train)\n",
    "\n",
    "y_pred = rclf2.predict(x_test)\n",
    "cfm = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "plt.figure(figsize=(10,6))\n",
    "plot_confusion_matrix(cfm, classes=[\"<=50K\", \">50K\"], normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，下面展示了对优化后模型比较重要的特征。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rclf2.feature_importances_\n",
    "indices = np.argsort(importances)\n",
    "cols = [cols[x] for x in indices]\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Feature Importances')\n",
    "plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "plt.yticks(range(len(indices)), cols)\n",
    "plt.xlabel('Relative Importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机森林的局限性\n",
    "\n",
    "除了 Bagging 树模型的一般局限性外，随机森林还有一些局限性：\n",
    "\n",
    "- 当我们需要推断超出范围的独立变量或非独立变量，随机森林做得并不好，我们最好使用如 MARS 那样的算法。\n",
    "- 随机森林算法在训练和预测时都比较慢。\n",
    "- 如果需要区分的类别十分多，随机森林的表现并不会很好。\n",
    "\n",
    "总的来说，随机森林在很多任务上一般要比提升方法的精度差，并且运行时间也更长。所有在 Kaggle 竞赛上，有很多模型都是使用的梯度提升树算法或其他优秀的提升方法。 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
